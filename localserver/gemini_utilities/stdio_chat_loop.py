# import libraries
import os

# import async libraries and  mcp client component
from typing import Optional
from aioconsole import ainput  # Async input library
from contextlib import AsyncExitStack
from mcp import ClientSession, StdioServerParameters  # for managing mcp sessions
from mcp.client.stdio import stdio_client  # mcp component for standard I/O

# import Gemini AI SDK
from google import genai
from google.genai import types
# from google.genai.types import GenerateContentConfig

# import enviroment loader
from dotenv import load_dotenv

# import utility script
from gemini_utilities.tools_converter import convert_mcp_tools_to_gemini
from gemini_utilities.system_input import system_instruction

# load env
dotenv_path = os.path.join(os.path.dirname(os.path.dirname(__file__)), '.env')
load_dotenv(dotenv_path=dotenv_path)

# server path
current_file_path = __file__  # Path to stdio_chat_loop.py
current_dir = os.path.dirname(current_file_path)
server_file_path = os.path.join(current_dir, "..", "ga4_server.py")


# build client
class MCPClient:
    def __init__(self):
        """ initialize the mcp client and configure the GEmimi API"""
        # mcp session for interaction
        self.session: Optional[ClientSession] = None
        # manage asyn cleanup
        self.exit_stack = AsyncExitStack()
        # To store conversation context
        self.conversation_history = []
        # get Gemini APi key from env
        gemini_api_key = os.getenv("GEMINI_API_KEY")
        if not gemini_api_key:
            raise ValueError("GEMINI_API_KEY not found, please add it to your .env file")
        # configure the Gemini AI client
        self.genai_client = genai.Client(api_key=gemini_api_key)
    
    async def connect_to_server(self, server_file_path: str):
        """connect to mcp server an list available tools

        Args:
            server_file_path (str): takes in the file path to server script
        """
        # determine the script language
        try:
            command = "python" if server_file_path.endswith(".py") else "node"
            # define the parameter for connecting the MCP server
            # check commands
            print(f"Command: {command}, Args: {server_file_path}")

            server_params = StdioServerParameters(command=command, args=[server_file_path])
            # establish connection with MCP server
            stdio_transport = await self.exit_stack.enter_async_context(
                stdio_client(server_params)
                )
            # extract the read and write transport object
            self.stdio, self.write = stdio_transport
            # initilize the mcp client
            self.session = await self.exit_stack.enter_async_context(
                ClientSession(self.stdio, self.write)
                )
            # send initilization request to the server
            await self.session.initialize()
            # request the list of available tools
            response = await self.session.list_tools()
            mcp_tools = response.tools  # extract the list of tools form  the response
            # print the name of the tools available in the server
            print("\nConnected to server with tools: ", [mcp_tool.name for mcp_tool in mcp_tools])
            # convert mcp tools to Gemini format and attache it to the class
            self.function_declaration = convert_mcp_tools_to_gemini(mcp_tools)
        except Exception as e:
            print(f"Failed to connect to MCP server or load tools: {e}")

    async def process_query(self, query: str):
        """process the user query using the geimin API and execute tool calls if needed

        Args:
            query (str): The user input query

        Returns:
            str: The respose generated by the Gemini model
        """
        # format user input as a structured content
        self.conversation_history.append({"role": "user", "content": query})

        # Convert the conversation history into Gemini-compatible structured content
        history_content = [
            types.Content(
                # indicate message source
                role=entry["role"],
                # convert text to gemini compatible format
                parts=[types.Part.from_text(text=entry["content"])]
            )
            for entry in self.conversation_history
        ]

        # send user input to Gemini AI along with the available tools
        response = self.genai_client.models.generate_content(
            # specify the model to use
            model="gemini-2.0-flash-001",
            # add user prompt
            contents=history_content,  # Include the entire history as context

            # pass the tools
            config=types.GenerateContentConfig(
                tools=self.function_declaration,
                system_instruction=system_instruction
                )
        )

        # initialize variable to store final resources
        final_text = []

        # process the response received from Gemini
        try:
            for candidate in response.candidates:
                # ensure response has content parts
                if candidate.content.parts:
                    # check if part is a valid Gemini response unit
                    for part in candidate.content.parts:
                        if isinstance(part, types.Part):
                            # check if part has function call
                            if part.function_call:
                                # extract the function call details
                                tool_name = part.function_call.name
                                tool_args = part.function_call.args
                                # print debug information
                                print(f"\n[Gemini requested toolcall:{tool_name} with args{tool_args}]")

                                # execute tool with MCP server
                                try:
                                    result = await self.session.call_tool(tool_name, arguments=tool_args)
                                    # store the output
                                    if isinstance(result.content, list):
                                        texts = [item.text for item in result.content if hasattr(item, "text")]
                                        tool_result_text = "\n".join(texts)
                                    else:
                                        tool_result_text = str(result.content)

                                    function_response = {"result": tool_result_text}
                                except Exception as e:
                                    function_response = {"error": str(e)}

                                # format the tool response for Gemini compatibility
                                function_response_part = types.Part.from_function_response(
                                    # name of the tool
                                    name=tool_name,
                                    # response gotten
                                    response=function_response
                                )

                                function_response_content = types.Content(
                                    role="tool",
                                    parts=[function_response_part]
                                )

                                print(f"Tool response :{function_response}")

                                try:
                                    # send the response and original prompt to the model
                                    response = self.genai_client.models.generate_content(
                                        model="gemini-2.0-flash-001",
                                        contents=[
                                            # attach original query
                                            *history_content,
                                            # create a clean assistant function call message
                                            types.Content(
                                                role="assistant",
                                                parts=[
                                                    types.Part.from_function_call(
                                                        name=tool_name,
                                                        args=tool_args
                                                    )
                                                ]
                                            ),
                                            # attach tool execution result
                                            function_response_content,
                                        ],
                                        config=types.GenerateContentConfig(
                                            tools=self.function_declaration,
                                            system_instruction=system_instruction
                                        ),
                                    )

                                    # extract response from Gemini after processing tool call
                                    try:
                                        follow_up = response.candidates[0].content.parts[0].text
                                        final_text.append(follow_up)
                                    except (IndexError, AttributeError) as e:
                                        error_msg = f"Error parsing Gemini response after tool call: {e}"
                                        print(error_msg)
                                        final_text.append(error_msg)

                                    # Add the assistant's message to conversation history
                                    self.conversation_history.append({
                                        "role": "assistant",
                                        "content": follow_up if 'follow_up' in locals() else ''
                                    })
                                except Exception as e:
                                    error_msg = f"Error generating Gemini follow-up after tool call: {e}"
                                    print(error_msg)
                                    final_text.append(error_msg)
                            else:
                                # if no tool call was requested, simply add Gemini text
                                final_text.append(part.text)
                                # add to conversation history
                                self.conversation_history.append({"role": "assistant", "content": part.text})

                            # return the final output
                            return "\n".join(final_text)
        except Exception as e:
            return f"Error processing Gemini response: {e}"
    
    async def chat_loop(self):
        """run  interaction chat session with user in the loop
        """

        while True:
            try:
                print("\nMCP client Active! Type 'quit' to exit, or 'clear' to reset the conversation.")
                query = await ainput("\nQuery: ")  #.strip()
                if query.lower() == 'quit':
                    break
                elif query.lower() == 'clear':
                    self.conversation_history = []  # Clear conversation history
                    print("Conversation history cleared.")
                    continue
                # processs the user's query
                response = await self.process_query(query)
                print(f"\nGemini: \u001b[32m{response}\u001b[0m")
            except Exception as e:
                print(f"\nAn unexpected error occurred during chat: {e}")
    
    async def clean_up(self):
        """Clean the resources before exiting
        """
        await self.exit_stack.aclose()
